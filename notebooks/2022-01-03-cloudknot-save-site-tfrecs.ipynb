{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cloudknot as ck\n",
    "import itertools\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tfrecs(sites, seed):\n",
    "    import nobrainer\n",
    "    import numpy as np\n",
    "    import os\n",
    "    import os.path as op\n",
    "    import pandas as pd\n",
    "    import re\n",
    "    \n",
    "    from glob import glob\n",
    "    from s3fs import S3FileSystem\n",
    "\n",
    "    fs = S3FileSystem()\n",
    "    bucket = \"hbn-pod2-deep-learning\"\n",
    "    output_s3_dir = \"/\".join([\"tfrecs\", \"_\".join(sites)])\n",
    "    s3_output_path = \"/\".join([bucket, output_s3_dir])\n",
    "\n",
    "    if fs.exists(\"/\".join([s3_output_path, \"_\".join(sites), f\"seed-{seed}\", \"train-subjects.csv\"])):\n",
    "        return\n",
    "\n",
    "    # Download the QC scores from S3 FCP-INDI\n",
    "    s3_participants = pd.read_csv(\n",
    "        \"s3://fcp-indi/data/Projects/HBN/BIDS_curated/derivatives/qsiprep/participants.tsv\",\n",
    "        sep=\"\\t\",\n",
    "        usecols=[\"subject_id\", \"scan_site_id\", \"expert_qc_score\", \"xgb_qc_score\"],\n",
    "        index_col=\"subject_id\",\n",
    "    )\n",
    "\n",
    "    # Download nifti files from S3 to local\n",
    "    local_nifti_dir = \"niftis\"\n",
    "    local_tfrec_dir = op.join(\"tfrecs\", \"_\".join(sites))\n",
    "    os.makedirs(local_nifti_dir, exist_ok=True)\n",
    "    os.makedirs(local_tfrec_dir, exist_ok=True)\n",
    "        \n",
    "    fs = S3FileSystem()\n",
    "    bucket = \"hbn-pod2-deep-learning\"\n",
    "    s3_input_dir = \"b0-tensorfa-dwiqc\"\n",
    "    s3_path = \"/\".join([bucket, s3_input_dir])\n",
    "    fs.get(s3_path, local_nifti_dir, recursive=True)\n",
    "\n",
    "    nifti_files = [op.abspath(filename) for filename in glob(f\"{local_nifti_dir}/*.nii.gz\")]\n",
    "    nifti_files = [fn for fn in nifti_files if \"irregularsize\" not in fn]\n",
    "    sub_id_pattern = re.compile(\"sub-[a-zA-Z0-9]*\")\n",
    "    subjects = [sub_id_pattern.search(s).group(0) for s in nifti_files]\n",
    "    \n",
    "    participants = pd.DataFrame(data=nifti_files, index=subjects, columns=[\"features\"])\n",
    "    participants = participants.merge(\n",
    "        s3_participants, left_index=True, right_index=True, how=\"left\"\n",
    "    )\n",
    "    participants.dropna(subset=[\"xgb_qc_score\"], inplace=True)\n",
    "    participants.rename(columns={\"xgb_qc_score\": \"labels\"}, inplace=True)\n",
    "\n",
    "    n_classes = 1\n",
    "    batch_size = 16\n",
    "    n_channels = 5\n",
    "    volume_shape = (128, 128, 128, n_channels)\n",
    "    block_shape = (128, 128, 128, n_channels)\n",
    "    num_parallel_calls = 4\n",
    "\n",
    "    # Get site inclusion indices for both the report set and\n",
    "    # the remaining sets, which we denote test becaume the\n",
    "    # train and validate sets will always be from different sites\n",
    "    # Take out the report set by checking for existence of expert QC scores\n",
    "    split_dataframes = {\n",
    "        \"report\": participants.loc[\n",
    "            np.logical_and(\n",
    "                participants[\"scan_site_id\"].isin(sites),\n",
    "                ~participants[\"expert_qc_score\"].isna(),\n",
    "            )\n",
    "        ],\n",
    "        \"test\": participants.loc[\n",
    "            np.logical_and(\n",
    "                participants[\"scan_site_id\"].isin(sites),\n",
    "                participants[\"expert_qc_score\"].isna(),\n",
    "            )\n",
    "        ],\n",
    "    }\n",
    "    \n",
    "    test_size = len(split_dataframes[\"test\"])\n",
    "    report_size = len(split_dataframes[\"report\"])\n",
    "\n",
    "    shuffled = split_dataframes[\"test\"].sample(\n",
    "        frac=1, random_state=seed\n",
    "    )\n",
    "    train_size = int(0.8 * test_size)\n",
    "    validate_size = test_size - train_size\n",
    "\n",
    "    split_dataframes[\"train\"] = shuffled.iloc[:train_size]\n",
    "    split_dataframes[\"validate\"] = shuffled.iloc[train_size:]\n",
    "\n",
    "    filepaths = {\n",
    "        split: list(df[[\"features\", \"labels\"]].itertuples(index=False, name=None))\n",
    "        for split, df in split_dataframes.items()\n",
    "    }\n",
    "    \n",
    "    # Verify that all volumes have the same shape\n",
    "    for split, fpaths in filepaths.items():\n",
    "        invalid = nobrainer.io.verify_features_labels(\n",
    "            fpaths, volume_shape=volume_shape, check_labels_int=False\n",
    "        )\n",
    "        print(f\"{split}, Invalid:\", invalid)\n",
    "        assert not invalid\n",
    "\n",
    "    # Save different sets of shuffled data\n",
    "    os.makedirs(local_tfrec_dir, exist_ok=True)\n",
    "\n",
    "    for split, fpaths in filepaths.items():\n",
    "        if split in [\"report\", \"test\"] and seed == 0:\n",
    "            write_path = op.join(\n",
    "                local_tfrec_dir,\n",
    "                f\"data-{'_'.join(sites)}-{split}\" + \"_shard-{shard:03d}.tfrec\",\n",
    "            )\n",
    "\n",
    "            print(f\"Writing {len(fpaths)} {split} TFRecords to {local_tfrec_dir}\")\n",
    "            nobrainer.tfrecord.write(\n",
    "                features_labels=fpaths,\n",
    "                filename_template=write_path,\n",
    "                examples_per_shard=2 * batch_size,\n",
    "            )\n",
    "        elif split in [\"train\", \"validate\"]:\n",
    "            os.makedirs(op.join(local_tfrec_dir, f\"seed-{seed}\"), exist_ok=True)\n",
    "            write_path = op.join(\n",
    "                local_tfrec_dir,\n",
    "                f\"seed-{seed}\",\n",
    "                f\"data-{'_'.join(sites)}-{split}\" + \"_shard-{shard:03d}.tfrec\",\n",
    "            )\n",
    "\n",
    "            print(f\"Writing {len(fpaths)} {split} TFRecords to {local_tfrec_dir}\")\n",
    "            nobrainer.tfrecord.write(\n",
    "                features_labels=fpaths,\n",
    "                filename_template=write_path,\n",
    "                examples_per_shard=2 * batch_size,\n",
    "            )\n",
    "\n",
    "    volume_numbers = pd.DataFrame([\n",
    "        {\"split\": \"report\", \"n_volumes\": report_size},\n",
    "        {\"split\": \"test\", \"n_volumes\": test_size},\n",
    "        {\"split\": \"train\", \"n_volumes\": train_size},\n",
    "        {\"split\": \"validate\", \"n_volumes\": validate_size},\n",
    "    ])\n",
    "    volume_numbers.set_index(\"split\", inplace=True, drop=True)\n",
    "    volume_numbers.to_csv(op.join(local_tfrec_dir, f\"seed-{seed}\", \"num_volumes.csv\"))\n",
    "\n",
    "    for split, df in split_dataframes.items():\n",
    "        if split in [\"report\", \"test\"] and seed == 0:\n",
    "            df.to_csv(op.join(local_tfrec_dir, f\"{split}-subjects.csv\"))\n",
    "        elif split in [\"train\", \"validate\"]:\n",
    "            os.makedirs(op.join(local_tfrec_dir, f\"seed-{seed}\"), exist_ok=True)\n",
    "            df.to_csv(op.join(local_tfrec_dir, f\"seed-{seed}\", f\"{split}-subjects.csv\"))\n",
    "\n",
    "    fs.put(\n",
    "        local_tfrec_dir,\n",
    "        s3_output_path,\n",
    "        recursive=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:cloudknot.dockerimage:Warning, your Dockerfile will have a base image of python:3, which may default to Python 3.8. This may cause dependency conflicts. If this build fails, consider rerunning with the `base_image='python:3.7' parameter.\n"
     ]
    }
   ],
   "source": [
    "di = ck.DockerImage(\n",
    "    func=create_tfrecs,\n",
    "    base_image=\"python:3.8\",\n",
    "    overwrite=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/richford/projects/neuro/hbn-pod2/qc/hbn-pod2-qc/notebooks/cloudknot_docker_create-tfrecs_23hm28mu'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "di.build_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "di.build(tags=[\"hbn-pod2-tfrecs-20220103\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo = ck.aws.DockerRepo(name=ck.get_ecr_repo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The very first time you run this, this command could take a few minutes\n",
    "di.push(repo=repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify bid_percentage to use Spot instances\n",
    "# And make sure the volume size is large enough. 55-60 GB seems about right for HBN preprocessing. YMMV.\n",
    "knot = ck.Knot(\n",
    "    name=f\"hbn-pod2-tfrecs-20220106-1\",\n",
    "    docker_image=di,\n",
    "    pars_policies=(\"AmazonS3FullAccess\",),\n",
    "    bid_percentage=100,\n",
    "    memory=8000,\n",
    "    job_def_vcpus=8,\n",
    "    volume_size=100,\n",
    "    max_vcpus=512,\n",
    "    retries=1,\n",
    "    aws_resource_tags={\"Project\": \"HBN-FCP-INDI\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['RU'], 0),\n",
       " (['RU'], 1),\n",
       " (['RU'], 2),\n",
       " (['RU'], 3),\n",
       " (['RU'], 4),\n",
       " (['RU'], 5),\n",
       " (['RU'], 6),\n",
       " (['RU'], 7),\n",
       " (['CBIC'], 0),\n",
       " (['CBIC'], 1),\n",
       " (['CBIC'], 2),\n",
       " (['CBIC'], 3),\n",
       " (['CBIC'], 4),\n",
       " (['CBIC'], 5),\n",
       " (['CBIC'], 6),\n",
       " (['CBIC'], 7),\n",
       " (['RU', 'CUNY'], 0),\n",
       " (['RU', 'CUNY'], 1),\n",
       " (['RU', 'CUNY'], 2),\n",
       " (['RU', 'CUNY'], 3),\n",
       " (['RU', 'CUNY'], 4),\n",
       " (['RU', 'CUNY'], 5),\n",
       " (['RU', 'CUNY'], 6),\n",
       " (['RU', 'CUNY'], 7),\n",
       " (['CBIC', 'CUNY'], 0),\n",
       " (['CBIC', 'CUNY'], 1),\n",
       " (['CBIC', 'CUNY'], 2),\n",
       " (['CBIC', 'CUNY'], 3),\n",
       " (['CBIC', 'CUNY'], 4),\n",
       " (['CBIC', 'CUNY'], 5),\n",
       " (['CBIC', 'CUNY'], 6),\n",
       " (['CBIC', 'CUNY'], 7)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seeds = np.arange(8)\n",
    "sites = [\n",
    "    [\"RU\"],\n",
    "    [\"CBIC\"],\n",
    "    [\"RU\", \"CUNY\"],\n",
    "    [\"CBIC\", \"CUNY\"],\n",
    "]\n",
    "args = list(itertools.product(\n",
    "    sites,\n",
    "    seeds,\n",
    "))\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = knot.map(args, starmap=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job ID              Name                        Status   \n",
      "---------------------------------------------------------\n",
      "ec550d4c-e4a5-41a1-b1b9-7be8377f53f1        hbn-pod2-tfrecs-20220106-1-5        PENDING  \n",
      "07e36f1a-c4d2-4dd9-b364-661987fe3d90        hbn-pod2-tfrecs-20220106-1-4        FAILED   \n",
      "71cec1bb-9948-450b-a097-b948e06abba3        hbn-pod2-tfrecs-20220106-1-2        FAILED   \n",
      "8bf78932-7d89-410b-be73-2772515a8ff1        hbn-pod2-tfrecs-20220106-1-3        FAILED   \n",
      "ba8ea13c-e99d-49b0-99d5-9d1d14657faa        hbn-pod2-tfrecs-20220106-1-1        SUCCEEDED\n",
      "31be5c6e-82e5-4a6c-ade7-c12a9068eb9e        hbn-pod2-tfrecs-20220106-1-0        SUCCEEDED\n"
     ]
    }
   ],
   "source": [
    "knot.view_jobs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "knot.clobber(clobber_pars=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
