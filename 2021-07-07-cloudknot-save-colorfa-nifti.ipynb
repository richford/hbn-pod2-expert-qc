{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/richford/miniconda3/envs/cloudknot-pyafq/lib/python3.8/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  data = yaml.load(f.read()) or {}\n"
     ]
    }
   ],
   "source": [
    "import cloudknot as ck\n",
    "import AFQ.data as afqd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving subject S3 keys\n",
      "[########################################] | 100% Completed |  0.5s\n"
     ]
    }
   ],
   "source": [
    "study = afqd.S3BIDSStudy(\n",
    "    study_id=\"hbn-qsiprep\",\n",
    "    bucket=\"fcp-indi\",\n",
    "    s3_prefix=\"data/Projects/HBN/BIDS_curated/derivatives/qsiprep\",\n",
    "    anon=True,\n",
    "    subjects=[1],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2136\n"
     ]
    }
   ],
   "source": [
    "subjects = study._all_subjects\n",
    "print(len(subjects))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_niftis(subject):\n",
    "    import AFQ.data as afqd\n",
    "    import bids\n",
    "    import numpy as np\n",
    "    import os\n",
    "    import os.path as op\n",
    "\n",
    "    from dipy.io.image import load_nifti, save_nifti\n",
    "    from dipy.io import read_bvals_bvecs\n",
    "    from dipy.core.gradients import gradient_table\n",
    "    import dipy.reconst.dti as dti\n",
    "    from dipy.align import resample\n",
    "    \n",
    "    from s3fs import S3FileSystem\n",
    "    \n",
    "    study = afqd.S3BIDSStudy(\n",
    "        study_id=\"hbn-qsiprep\",\n",
    "        bucket=\"fcp-indi\",\n",
    "        s3_prefix=\"data/Projects/HBN/BIDS_curated/derivatives/qsiprep\",\n",
    "        anon=True,\n",
    "        subjects=[subject]\n",
    "    )\n",
    "    \n",
    "    local_bids_folder = \"hbn\"\n",
    "    output_bucket = \"hbn-pod2-deep-learning\"\n",
    "    study.download(local_bids_folder)\n",
    "    layout = bids.BIDSLayout(local_bids_folder, validate=False)\n",
    "    \n",
    "    subject = subject.replace(\"sub-\", \"\")        \n",
    "\n",
    "    bids_filters = {\"subject\": subject, \"return_type\": \"filename\"}\n",
    "\n",
    "    fdwi = layout.get(extension=\"nii.gz\", suffix=\"dwi\", **bids_filters)[0]\n",
    "    fb0 = layout.get(suffix=\"dwiref\", extension=\"nii.gz\", **bids_filters)[0]\n",
    "    ft1w = layout.get(suffix=\"T1w\", extension=\"nii.gz\", space=None, **bids_filters)[0]\n",
    "    fmask = layout.get(suffix=\"mask\", datatype=\"dwi\", extension=\"nii.gz\", **bids_filters)[0]\n",
    "    fwm = layout.get(suffix=\"probseg\", space=None, extension=\"nii.gz\", **bids_filters)\n",
    "    fwm = [fn for fn in fwm if \"label-WM\" in fn][0]\n",
    "\n",
    "    t1w_data, t1w_affine = load_nifti(ft1w)\n",
    "    mask_data, mask_affine = load_nifti(fmask)\n",
    "    b0_data, b0_affine = load_nifti(fb0)\n",
    "\n",
    "    data, affine = load_nifti(fdwi)\n",
    "    wm_data, wm_affine = load_nifti(fwm)\n",
    "    t1w_dwi = resample(t1w_data, data[:, :, :, 0], t1w_affine, affine)\n",
    "    wm_dwi = resample(wm_data, data[:, :, :, 0], wm_affine, affine)\n",
    "    mask_dwi = resample(mask_data, data[:, :, :, 0], mask_affine, affine)\n",
    "    b0_dwi = resample(b0_data, data[:, :, :, 0], b0_affine, affine)\n",
    "\n",
    "    fbval = layout.get_bval(path=fdwi, subject=subject)\n",
    "    fbvec = layout.get_bvec(path=fdwi, subject=subject)\n",
    "    bvals, bvecs = read_bvals_bvecs(fbval, fbvec)\n",
    "    gtab = gradient_table(bvals, bvecs)\n",
    "\n",
    "    tenmodel = dti.TensorModel(gtab)\n",
    "    tenfit = tenmodel.fit(data)\n",
    "    FA = dti.fractional_anisotropy(tenfit.evals)\n",
    "    FA = np.clip(FA, 0, 1)\n",
    "\n",
    "    FA_masked = FA * wm_dwi.get_fdata()\n",
    "    RGB = dti.color_fa(FA_masked, tenfit.evecs)\n",
    "\n",
    "    RGB = np.array(255 * RGB, 'uint8')\n",
    "\n",
    "    def trim_zeros(arr, margin=0, trim_dims=None):\n",
    "        '''\n",
    "        Trim the leading and trailing zeros from a N-D array.\n",
    "\n",
    "        :param arr: numpy array\n",
    "        :param margin: how many zeros to leave as a margin\n",
    "        :returns: trimmed array\n",
    "        :returns: slice object\n",
    "        '''\n",
    "        s = []\n",
    "        if trim_dims is None:\n",
    "            trim_dims = list(range(arr.ndim))\n",
    "\n",
    "        for dim in range(arr.ndim):\n",
    "            start = 0\n",
    "            end = -1\n",
    "\n",
    "            if dim in trim_dims:\n",
    "                slice_ = [slice(None)]*arr.ndim\n",
    "\n",
    "                go = True\n",
    "                while go:\n",
    "                    slice_[dim] = start\n",
    "                    go = not np.any(arr[tuple(slice_)])\n",
    "                    start += 1\n",
    "                start = max(start-1-margin, 0)\n",
    "\n",
    "                go = True\n",
    "                while go:\n",
    "                    slice_[dim] = end\n",
    "                    go = not np.any(arr[tuple(slice_)])\n",
    "                    end -= 1\n",
    "                end = arr.shape[dim] + min(-1, end+1+margin) + 1\n",
    "\n",
    "                s.append(slice(start,end))\n",
    "            else:\n",
    "                s.append(slice(None, None, None))\n",
    "        return arr[tuple(s)], tuple(s)\n",
    "\n",
    "    def pad_volume_to_128(arr, dim_max=128):\n",
    "        dim_x, dim_y, dim_z = arr.shape[0], arr.shape[1], arr.shape[2]\n",
    "        pad_xr = (dim_max - dim_x) // 2\n",
    "        pad_xl = dim_max - dim_x - pad_xr\n",
    "        pad_yr = (dim_max - dim_y) // 2\n",
    "        pad_yl = dim_max - dim_y - pad_yr\n",
    "        pad_zr = (dim_max - dim_z) // 2\n",
    "        pad_zl = dim_max - dim_z - pad_zr\n",
    "\n",
    "        pad_width = [(pad_xl, pad_xr), (pad_yl, pad_yr), (pad_zl, pad_zr)]\n",
    "        for i in range(arr.ndim - 3):\n",
    "            pad_width.append((0, 0))\n",
    "        \n",
    "        return np.pad(arr, pad_width=pad_width)\n",
    "\n",
    "    mask_trim, trim_slices = trim_zeros(mask_dwi.get_fdata(), margin=5, trim_dims=(0, 1))\n",
    "    t1w_dwi = t1w_dwi.get_fdata()[trim_slices]\n",
    "    RGB = RGB[trim_slices + (slice(None, None, None),)]\n",
    "    FA_masked = FA_masked[trim_slices]\n",
    "    b0_dwi = b0_dwi.get_fdata()[trim_slices]\n",
    "\n",
    "    is_128 = True\n",
    "    \n",
    "    try:\n",
    "        RGB = pad_volume_to_128(RGB)\n",
    "        FA_masked = pad_volume_to_128(FA_masked.astype(np.float32))\n",
    "        b0_dwi = pad_volume_to_128(b0_dwi)\n",
    "        is_128 = True\n",
    "    except ValueError:\n",
    "        dim_max = np.max([\n",
    "            np.max(b0_dwi.shape),\n",
    "            np.max(FA_masked.shape),\n",
    "            np.max(RGB.shape), \n",
    "        ])\n",
    "        RGB = pad_volume_to_128(RGB, dim_max=dim_max)\n",
    "        FA_masked = pad_volume_to_128(FA_masked.astype(np.float32), dim_max=dim_max)\n",
    "        b0_dwi = pad_volume_to_128(b0_dwi, dim_max=dim_max)\n",
    "        is_128 = False\n",
    "\n",
    "    combined = np.concatenate([\n",
    "        RGB / 255,\n",
    "        b0_dwi[..., np.newaxis] / np.max(b0_dwi)\n",
    "    ], axis=-1)\n",
    "\n",
    "    # Save and upload to S3\n",
    "    fname_combined = f\"sub-{subject}_combined.nii.gz\"\n",
    "    if is_128:\n",
    "        save_nifti(fname_combined, combined, affine)\n",
    "        fs = S3FileSystem(anon=False)\n",
    "        fs.put(fname_combined, \"/\".join([output_bucket, \"combined\", op.basename(fname_combined)]))\n",
    "    else:\n",
    "        fname_combined = f\"sub-{subject}_desc-irregularsize_b0colorfa.nii.gz\"\n",
    "        save_nifti(fname_combined, combined, affine)\n",
    "        fs = S3FileSystem(anon=False)\n",
    "        fs.put(fname_combined, \"/\".join([output_bucket, \"combined\", op.basename(fname_combined)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:cloudknot.dockerimage:Warning, your Dockerfile will have a base image of python:3, which may default to Python 3.8. This may cause dependency conflicts. If this build fails, consider rerunning with the `base_image='python:3.7' parameter.\n"
     ]
    }
   ],
   "source": [
    "di = ck.DockerImage(\n",
    "    func=create_niftis,\n",
    "    base_image=\"python:3.8\",\n",
    "    github_installs=\"https://github.com/yeatmanlab/pyAFQ.git@master\",\n",
    "    overwrite=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "di.build(tags=[\"hbn-pod2-niftis-20210720\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo = ck.aws.DockerRepo(name=ck.get_ecr_repo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The very first time you run this, this command could take\n",
    "# a few hours because the docker image is large\n",
    "di.push(repo=repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify bid_percentage to use Spot instances\n",
    "# And make sure the volume size is large enough. 55-60 GB seems about right for HBN preprocessing. YMMV.\n",
    "knot = ck.Knot(\n",
    "    name=f\"hbn-pod2-niftis-20210720-1\",\n",
    "    docker_image=di,\n",
    "    pars_policies=(\"AmazonS3FullAccess\",),\n",
    "    bid_percentage=100,\n",
    "    memory=8000,\n",
    "    job_def_vcpus=4,\n",
    "    max_vcpus=4096,\n",
    "    retries=3,\n",
    "    aws_resource_tags={\"Project\": \"HBN-FCP-INDI\"},\n",
    ")\n",
    "\n",
    "# Retrieve the above knot from config file\n",
    "# knot = ck.Knot(name=f\"fibr-gifs-20201116-0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pilot_subs = subjects[:10]\n",
    "production_subs = subjects[10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = knot.map(pilot_subs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job ID              Name                        Status   \n",
      "---------------------------------------------------------\n",
      "662a71ee-0aca-42a4-93ac-8624cfed7d24        hbn-pod2-niftis-20210720-1-0        FAILED   \n",
      "ef36175d-1684-4276-b588-93f8a9ac19e4        hbn-pod2-niftis-20210720-1-1        SUCCEEDED\n",
      "ef850e26-6eed-4a15-bbb6-cecd648822be        hbn-pod2-niftis-20210720-1-2        SUCCEEDED\n"
     ]
    }
   ],
   "source": [
    "knot.view_jobs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = knot.map(production_subs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "knot.clobber(clobber_pars=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
